
\subsection{Декодер}

\begin{frame}[allowframebreaks]
	\frametitle{\om Детали работы декодера}
	\begin{itemize}
	\item \textbf{В первом} режиме работы принимается исходный текст.
		\begin{itemize}
			\item Последовательно разбивается на $n$-граммы.
			\begin{itemize}
				\item Сначала наибольшего размера.
			\end{itemize}
			\item $n$-граммы ищутся в базе данных.
			\begin{itemize}
				\item Если нашли, выбираем наиболее вероятную.
				\item Если нет, берем $n$-грамму меньшего размера, 
					слова ($1$-граммы) возвращаем как есть.
			\end{itemize}
			\item Вычисляем величину неопределенности.
		\end{itemize}
	\item \textbf{Во втором} режиме работы на вход принимается:
		\begin{itemize}
			\item исходный текст (ИТ); 
			\item переводной текст (ПТ) c~предыдущей итерации
			\item величина неопределенности (ВН).
		\end{itemize}
	\end{itemize}
	\pagebreak
		\[
			\text{ВН} = 2^{- \left( \frac{1}{S_{\NE}}\sum\limits_{i = 1}^{S_{\NE}} \log_2 P(\NE_i) 
				+ \frac{1}{S_{\WE}}\sum\limits_{j = 1}^{S_{\WE}} \log_2 P(\WR_j|\WE_j) \right) } 
		\]
		\begin{itemize}
			\item $\NE$ --- $n$-граммы найденные в созданном тексте;
			\item $S_{\NE}$ --- количество таких $n$-грамм;
			\item $P(\NE)$ --- вероятность $n$-грамм согласно языковой модели (вычисляется как указано раннее);
			\item $\WE$ --- $n$-граммы (слова) как результат перевода согласно модели перевода;
			\item $S_{\WE}$ --- количество таких $n$-грамм (слов);
			\item $P(\WR_j|\WE_j)$ --- вероятность перевода фразы $\WE_j$ на $\WR_j$.
		\end{itemize}
\end{frame}
